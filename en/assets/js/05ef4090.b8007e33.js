"use strict";(self.webpackChunkutm_doc=self.webpackChunkutm_doc||[]).push([[4912],{2454:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"Algorithm_Development/SLAM/ORB-SLAM3/ORB-SLAM3","title":"ORB-SLAM3","description":"Source//github.com/UZ-SLAMLab/ORB\\\\SLAM3","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/09_Algorithm_Development/04_SLAM/ORB-SLAM3/ORB-SLAM3.md","sourceDirName":"09_Algorithm_Development/04_SLAM/ORB-SLAM3","slug":"/Algorithm_Development/SLAM/ORB-SLAM3/","permalink":"/en/Algorithm_Development/SLAM/ORB-SLAM3/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"source":"https://wiki.utmsys.org/Algorithm_Development/SLAM/ORB-SLAM3/"},"sidebar":"tutorialSidebar","previous":{"title":"FAST-LIO","permalink":"/en/Algorithm_Development/SLAM/FAST-LIO/"},"next":{"title":"RTAB-MAP ","permalink":"/en/Algorithm_Development/SLAM/RTAB-MAP/"}}');var i=r(4848),s=r(8453);const t={source:"https://wiki.utmsys.org/Algorithm_Development/SLAM/ORB-SLAM3/"},o=void 0,l={},d=[{value:"V1.0, December",id:"v10-december",level:3},{value:"Related publications",id:"related-publications",level:3},{value:"1. License",id:"1-license",level:2},{value:"2. Prerequisites",id:"2-prerequisites",level:2},{value:"C++11 or C++0x",id:"c11-or-c0x",level:2},{value:"Pangolin",id:"pangolin",level:2},{value:"OpenCV",id:"opencv",level:2},{value:"Feature",id:"feature",level:2},{value:"DBoW2 and g2o (included in the third-party folder",id:"dbow2-and-g2o-included-in-the-third-party-folder",level:2},{value:"Python",id:"python",level:2},{value:"ROS (optional",id:"ros-optional",level:2},{value:"3. Build ORB-SLAM3 library and examples",id:"3-build-orb-slam3-library-and-examples",level:2},{value:"4. Run ORB-SLAM3 with a camera",id:"4-run-orb-slam3-with-a-camera",level:2},{value:"5. EuRoC Example",id:"5-euroc-example",level:2},{value:"Evaluation",id:"evaluation",level:2},{value:"6. TUM-VI Example",id:"6-tum-vi-example",level:2},{value:"Evaluation",id:"evaluation-1",level:2},{value:"7. ROS Examples",id:"7-ros-examples",level:2},{value:"Build Mono, Mono Inertial, Stereo, Stereo Inertial, and RGB-D",id:"build-mono-mono-inertial-stereo-stereo-inertial-and-rgb-d",level:3},{value:"Running a Monocular",id:"running-a-monocular",level:3},{value:"Running the Monocular Inertial",id:"running-the-monocular-inertial",level:3},{value:"Running the Stereo",id:"running-the-stereo",level:3},{value:"Run the Stereo Inertial",id:"run-the-stereo-inertial",level:3},{value:"Run the RGB_D",id:"run-the-rgb_d",level:3},{value:"8. Runtime Analysis",id:"8-runtime-analysis",level:2},{value:"9. Calibration",id:"9-calibration",level:2}];function c(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["Source: ",(0,i.jsx)(n.a,{href:"https://github.com/UZ-SLAMLab/ORB_SLAM3",children:"https://github.com/UZ-SLAMLab/ORB_SLAM3"})]}),"\n",(0,i.jsx)(n.h3,{id:"v10-december",children:"V1.0, December"}),"\n",(0,i.jsxs)(n.p,{children:["**Authors:** Carlos Campos, Richard Elvira, Juan J. G\xf3mez Rodr\xedguez, ",(0,i.jsx)(n.a,{href:"http://webdiis.unizar.es/~josemari/",children:"Jos\xe9 MM Montiel"}),", ",(0,i.jsx)(n.a,{href:"http://webdiis.unizar.es/~jdtardos/",children:"Juan D. Tardos"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Changelog.md",children:"The changelog"})," describes the features of each version."]}),"\n",(0,i.jsxs)(n.p,{children:["ORB-SLAM3 is the first real-time SLAM library capable of performing ",(0,i.jsx)(n.strong,{children:"visual, visual-inertial, and multi-map SLAM using"})," ",(0,i.jsx)(n.strong,{children:"monocular, stereo, and RGB-D"})," cameras, supporting ",(0,i.jsx)(n.strong,{children:"both pinhole and fisheye"})," lens models. Across all sensor configurations, ORB-SLAM3 achieves robustness comparable to the best systems in the literature while significantly improving accuracy."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",children:"We provide"})," examples of running ORB-SLAM3 using stereo or monocular (including IMU and monocular) on the EuRoC dataset, and using fisheye stereo or monocular (including IMU and monocular) on ",(0,i.jsx)(n.a,{href:"https://vision.in.tum.de/data/datasets/visual-inertial-dataset",children:"the TUM-VI dataset. Videos of some of these examples can be found on"})," ",(0,i.jsx)(n.a,{href:"https://www.youtube.com/channel/UCXVt-kXG6T95Z4tVaYlU80Q",children:"the ORB-SLAM3 channel"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["This software is based on ",(0,i.jsx)(n.a,{href:"https://github.com/raulmur/ORB_SLAM2",children:"ORB-SLAM2"})," developed by ",(0,i.jsx)(n.a,{href:"http://webdiis.unizar.es/~raulmur/",children:"Raul Mur-Artal"}),", ",(0,i.jsx)(n.a,{href:"http://webdiis.unizar.es/~jdtardos/",children:"Juan D. Tardos"}),", ",(0,i.jsx)(n.a,{href:"http://webdiis.unizar.es/~josemari/",children:"JMM Montiel"}),", and ",(0,i.jsx)(n.a,{href:"http://doriangalvez.com/",children:"Dorian Galvez-Lopez"})," ( ",(0,i.jsx)(n.a,{href:"https://github.com/dorian3d/DBoW2",children:"DBoW2"})," )."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://youtu.be/HyLNq-98LRo",children:(0,i.jsx)(n.img,{src:"https://camo.githubusercontent.com/1b85904badac830a45287a07cc68d4473141c343248512c51ee77324ad91112a/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f48794c4e712d39384c526f2f302e6a7067",alt:"ORB-SLAM3"})})}),"\n",(0,i.jsx)(n.h3,{id:"related-publications",children:"Related publications"}),"\n",(0,i.jsxs)(n.p,{children:["[ORB-SLAM3] Carlos Campos, Richard Elvira, Juan J. G\xf3mez Rodr\xedguez, Jos\xe9 MM Montiel, and Juan D. Tard\xf3s. ",(0,i.jsx)(n.strong,{children:"ORB-SLAM3: An accurate open-source library for visual, visual-inertial, and multi-map SLAM"}),". IEEE ",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2007.11898",children:"Transactions on Robotics"})," ",(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.em,{children:"37(6):1874\u20131890, December 2021. PDF"}),"."]}),"**"]}),"\n",(0,i.jsxs)(n.p,{children:["[IMU Initialization] Carlos Campos, JMM Montiel, and Juan D. Tard\xf3s, ",(0,i.jsx)(n.strong,{children:"Pure Inertial Optimization for Visual-Inertial Initialization"}),", ",(0,i.jsx)(n.em,{children:"ICRA"})," 2020. ",(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"https://arxiv.org/pdf/2003.05766.pdf",children:"PDF"})})]}),"\n",(0,i.jsxs)(n.p,{children:["[ORBSLAM-Atlas] Richard Elvira, J.M.M. Montiel, and Juan D. Tard\xf3s, ORBSLAM- ",(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.a,{href:"https://arxiv.org/pdf/1908.11585.pdf",children:"Atlas"})," ****: A Robust and Accurate Multi-Map System"]}),", ",(0,i.jsx)(n.em,{children:"IROS 2019."})," PDF."]}),"\n",(0,i.jsxs)(n.p,{children:["[ORBSLAM-VI] Ra\xfal Mur-Artal and Juan D. Tard\xf3s. ",(0,i.jsx)(n.strong,{children:"Visual-Inertial Monocular SLAM with Reusable Maps"}),". IEEE Robotics and Automation Letters, vol. 2, no. 2, pp. 796\u2013803, 2017. PDF ",(0,i.jsx)(n.a,{href:"https://arxiv.org/pdf/1610.05949.pdf",children:"."})]}),"\n",(0,i.jsxs)(n.p,{children:["[Stereo and RGB-D] Ra\xfal Mur-Artal and Juan D. Tard\xf3s. ORB ",(0,i.jsx)(n.strong,{children:"-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-D cameras"}),". IEEE Transactions on Robotics, Vol. 33, No. 5, pp. 1255\u20131262, 2017. PDF ",(0,i.jsx)(n.a,{href:"https://arxiv.org/pdf/1610.06475.pdf",children:"."})]}),"\n",(0,i.jsxs)(n.p,{children:["[Monocular] Ra\xfal Mur-Artal, Jos\xe9 M. M. Montiel, and Juan D. Tard\xf3s. ORB ",(0,i.jsx)(n.strong,{children:"-SLAM: A Versatile and Accurate Monocular SLAM System"}),". IEEE Transactions on Robotics, Vol. 31, No. 5, pp. 1147\u20131163, 2015. ( ",(0,i.jsx)(n.strong,{children:"2015 IEEE Transactions on Robotics Best Paper Award"})," ) ",(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"https://arxiv.org/pdf/1502.00956.pdf",children:". PDF"})}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["[DBoW2 Place Recognition] Dorian G\xe1lvez-L\xf3pez and Juan D. Tard\xf3s. ",(0,i.jsx)(n.strong,{children:"Fast Place Recognition in Image Sequences Based on Binary Bag of Words"}),". IEEE Transactions on Robotics, Vol. 28, No. 5, pp. 1188\u20131197, ",(0,i.jsx)(n.a,{href:"http://doriangalvez.com/php/dl.php?dlp=GalvezTRO12.pdf",children:"2012."})," PDF"]}),"\n",(0,i.jsx)(n.h2,{id:"1-license",children:"1. License"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/UZ-SLAMLab/ORB_SLAM3/LICENSE",children:"ORB-SLAM3"})," is released under the GPLv3 license. For a list of all code/library dependencies (and their associated licenses), see ",(0,i.jsx)(n.a,{href:"https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Dependencies.md",children:"Dependencies.md"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"To obtain a closed-source version of ORB-SLAM3 for commercial use, please contact the  orbslam (at) unizar (dot) es."}),"\n",(0,i.jsx)(n.p,{children:"If you use ORB-SLAM3 in your academic work, please cite:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"@article{ORBSLAM3_TRO,\r\n  title={{ORB-SLAM3}: An Accurate Open-Source Library for Visual, Visual-Inertial \r\n           and Multi-Map {SLAM}},\r\n  author={Campos, Carlos AND Elvira, Richard AND G\\\xb4omez, Juan J. AND Montiel, \r\n          Jos\\'e M. M. AND Tard\\'os, Juan D.},\r\n  journal={IEEE Transactions on Robotics}, \r\n  volume={37},\r\n  number={6},\r\n  pages={1874-1890},\r\n  year={2021}\r\n }\n"})}),"\n",(0,i.jsx)(n.h2,{id:"2-prerequisites",children:"2. Prerequisites"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"We have tested the library in Ubuntu 16.04"})," and ",(0,i.jsx)(n.strong,{children:"18.04"}),", but it should be easy to compile on other platforms as well. A powerful computer (e.g. an i7) will ensure real-time performance and provide more stable and accurate results."]}),"\n",(0,i.jsx)(n.h2,{id:"c11-or-c0x",children:"C++11 or C++0x"}),"\n",(0,i.jsx)(n.p,{children:"We use the new threading and timing features of C++11."}),"\n",(0,i.jsx)(n.h2,{id:"pangolin",children:"Pangolin"}),"\n",(0,i.jsxs)(n.p,{children:["We use ",(0,i.jsx)(n.a,{href:"https://github.com/stevenlovegrove/Pangolin",children:"Pangolin"})," for visualization and user interface. Download and installation instructions can be found at: ",(0,i.jsx)(n.a,{href:"https://github.com/stevenlovegrove/Pangolin",children:"https://github.com/stevenlovegrove/Pangolin"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"opencv",children:"OpenCV"}),"\n",(0,i.jsxs)(n.p,{children:["We use ",(0,i.jsx)(n.a,{href:"http://opencv.org/",children:"OpenCV"})," to process images and features. Download and installation instructions can be found at ",(0,i.jsx)(n.a,{href:"http://opencv.org/",children:"http://opencv.org"}),". ",(0,i.jsx)(n.strong,{children:"At least version 3.0 is required. We have tested it with OpenCV 3.2.0 and 4.4.0"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"feature",children:"Feature"}),"\n",(0,i.jsxs)(n.p,{children:["Required for g2o (see below). Download and installation instructions are available at: ",(0,i.jsx)(n.a,{href:"http://eigen.tuxfamily.org/",children:"http://eigen.tuxfamily.org"}),". ",(0,i.jsx)(n.strong,{children:"At least version 3.1.0 is required"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"dbow2-and-g2o-included-in-the-third-party-folder",children:"DBoW2 and g2o (included in the third-party folder"}),"\n",(0,i.jsxs)(n.p,{children:["We use a modified version of the ",(0,i.jsx)(n.a,{href:"https://github.com/dorian3d/DBoW2",children:"DBoW2"})," library for position recognition and the ",(0,i.jsx)(n.a,{href:"https://github.com/RainerKuemmerle/g2o",children:"g2o"})," library for nonlinear optimization. Both modified versions (BSD versions) are included in the _Thirdparty_ folder."]}),"\n",(0,i.jsx)(n.h2,{id:"python",children:"Python"}),"\n",(0,i.jsxs)(n.p,{children:["Used to compute alignment of trajectories with ground truth. ",(0,i.jsx)(n.strong,{children:"Requires Numpy module"}),"."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\uff08win\uff09 ",(0,i.jsx)(n.a,{href:"http://www.python.org/downloads/windows",children:"http://www.python.org/downloads/windows"})]}),"\n",(0,i.jsxs)(n.li,{children:["(say) ",(0,i.jsx)(n.code,{children:"sudo apt install libpython2.7-dev"})]}),"\n",(0,i.jsx)(n.li,{children:"(Mac) Pre-installed OSX"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"ros-optional",children:"ROS (optional"}),"\n",(0,i.jsx)(n.p,{children:"We provide some examples for processing monocular, monocular-inertial, stereo, stereo-inertial, or RGB-D camera input using ROS. Building these examples is optional. These examples have been tested using ROS Melodic on an Ubuntu 18.04 system."}),"\n",(0,i.jsx)(n.h2,{id:"3-build-orb-slam3-library-and-examples",children:"3. Build ORB-SLAM3 library and examples"}),"\n",(0,i.jsx)(n.p,{children:"Clone the repository:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"git clone https://github.com/UZ-SLAMLab/ORB_SLAM3.git ORB_SLAM3\n"})}),"\n",(0,i.jsxs)(n.p,{children:["We provide a script ",(0,i.jsx)(n.code,{children:"build.sh"})," to build the third-party libraries and ORB-SLAM3. Please make sure you have installed all required dependencies (see Section 2). Execute the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"cd ORB_SLAM3\r\nchmod +x build.sh\r\n./build.sh\n"})}),"\n",(0,i.jsxs)(n.p,{children:["This will create libORB_SLAM3.so in the _lib folder ",(0,i.jsx)(n.strong,{children:"and"})," the executable in the _Examples_ folder."]}),"\n",(0,i.jsx)(n.h2,{id:"4-run-orb-slam3-with-a-camera",children:"4. Run ORB-SLAM3 with a camera"}),"\n",(0,i.jsxs)(n.p,{children:["The directory ",(0,i.jsx)(n.code,{children:"Examples"})," contains several demo programs and calibration files for running ORB-SLAM3 in all sensor configurations with Intel Realsense cameras T265 and D435i. The steps required to use your own camera are as follows:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Calibrate camera tracking ",(0,i.jsx)(n.code,{children:"Calibration_Tutorial.pdf"})," and write calibration files ",(0,i.jsx)(n.code,{children:"your_camera.yaml"})]}),"\n",(0,i.jsx)(n.li,{children:"Modify one of the provided demos to suit your specific camera model, then build it"}),"\n",(0,i.jsx)(n.li,{children:"Connect the camera to the computer using USB3 or an appropriate interface"}),"\n",(0,i.jsx)(n.li,{children:"Run ORB-SLAM3. For example, for our D435i camera, we would execute:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"./Examples/Stereo-Inertial/stereo_inertial_realsense_D435i Vocabulary/ORBvoc.txt ./Examples/Stereo-Inertial/RealSense_D435i.yaml\n"})}),"\n",(0,i.jsx)(n.h2,{id:"5-euroc-example",children:"5. EuRoC Example"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",children:"The EuRoC dataset"})," was recorded with two pinhole cameras and one inertial sensor. We provide a sample script to start a EuRoC sequence in all sensor configurations."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Download the sequences (ASL format) ",(0,i.jsx)(n.a,{href:"http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",children:"from http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets"})]}),"\n",(0,i.jsxs)(n.li,{children:['Open the script "euroc_examples.sh" in the project root directory. Change the ',(0,i.jsx)(n.strong,{children:"pathDatasetEuroc"})," variable to point to the directory where the dataset is unpacked."]}),"\n",(0,i.jsx)(n.li,{children:"Execute the following script to process all sequences for all sensor configurations:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"./euroc_examples\n"})}),"\n",(0,i.jsx)(n.h2,{id:"evaluation",children:"Evaluation"}),"\n",(0,i.jsx)(n.p,{children:'EuRoC provides ground truth for each sequence in the IMU body reference. Since the trajectories reported by the vision-only implementation are centered on the left camera, we provide the ground truth to left-camera reference transformation in the "evaluation" folder. The visual-inertial trajectories use the ground truth from the dataset.'}),"\n",(0,i.jsx)(n.p,{children:"Execute the following script to process the sequence and calculate the RMS ATE:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"./euroc_eval_examples\n"})}),"\n",(0,i.jsx)(n.h2,{id:"6-tum-vi-example",children:"6. TUM-VI Example"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://vision.in.tum.de/data/datasets/visual-inertial-dataset",children:"The TUM-VI dataset"})," is recorded by two fisheye cameras and one inertial sensor."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://vision.in.tum.de/data/datasets/visual-inertial-dataset",children:"Download a sequence from https://vision.in.tum.de/data/datasets/visual-inertial-dataset"})," and unzip it."]}),"\n",(0,i.jsxs)(n.li,{children:['Open the script "tum_vi_examples.sh" in the project root directory and change the ',(0,i.jsx)(n.strong,{children:"pathDatasetTUM_VI"})," variable to point to the directory where the dataset is unzipped."]}),"\n",(0,i.jsx)(n.li,{children:"Execute the following script to process all sequences for all sensor configurations:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"./tum_vi_examples\n"})}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-1",children:"Evaluation"}),"\n",(0,i.jsx)(n.p,{children:"In TUM-VI, ground truth is only available in the rooms where all sequences start and end. Therefore, the error measures the drift at the end of the sequence."}),"\n",(0,i.jsx)(n.p,{children:"Execute the following script to process the sequence and calculate the RMS ATE:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"./tum_vi_eval_examples\n"})}),"\n",(0,i.jsx)(n.h2,{id:"7-ros-examples",children:"7. ROS Examples"}),"\n",(0,i.jsx)(n.h3,{id:"build-mono-mono-inertial-stereo-stereo-inertial-and-rgb-d",children:"Build Mono, Mono Inertial, Stereo, Stereo Inertial, and RGB-D"}),"\n",(0,i.jsx)(n.p,{children:"Tested using ROS Melodic and Ubuntu 18.04."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"_Add the path containing Examples/ROS/ORB_SLAM3_ to the ROS_PACKAGE_PATH environment variable. Open the.bashrc file:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"gedit ~/.bashrc\n"})}),"\n",(0,i.jsx)(n.p,{children:"and add the following line at the end. Replace PATH with the folder where you cloned ORB_SLAM3:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"export ROS_PACKAGE_PATH=${ROS_PACKAGE_PATH}:PATH/ORB_SLAM3/Examples/ROS\n"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Execute ",(0,i.jsx)(n.code,{children:"build_ros.sh"})," the script:"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"chmod +x build_ros.sh\r\n./build_ros.sh\n"})}),"\n",(0,i.jsx)(n.h3,{id:"running-a-monocular",children:"Running a Monocular"}),"\n",(0,i.jsxs)(n.p,{children:["For monocular input from a subject, ",(0,i.jsx)(n.code,{children:"/camera/image_raw"})," run the node ORB_SLAM3/Mono. You will need to provide a vocabulary file and a settings file. See the monocular example above."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"rosrun ORB_SLAM3 Mono PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE\n"})}),"\n",(0,i.jsx)(n.h3,{id:"running-the-monocular-inertial",children:"Running the Monocular Inertial"}),"\n",(0,i.jsxs)(n.p,{children:["For the monocular input from the subject ",(0,i.jsx)(n.code,{children:"/camera/image_raw"})," and the inertial input from the subject ",(0,i.jsx)(n.code,{children:"/imu"}),", run the node ORB_SLAM3/Mono_Inertial. Setting the optional third parameter to true will apply CLAHE equalization to the image (primarily for the TUM-VI dataset)."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"rosrun ORB_SLAM3 Mono PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE [EQUALIZATION]\n"})}),"\n",(0,i.jsx)(n.h3,{id:"running-the-stereo",children:"Running the Stereo"}),"\n",(0,i.jsxs)(n.p,{children:["For stereo input from a subject ",(0,i.jsx)(n.code,{children:"/camera/left/image_raw"}),", ",(0,i.jsx)(n.code,{children:"/camera/right/image_raw"})," run the node ORB_SLAM3/Stereo. You will need to provide a vocabulary file and a settings file. For pinhole camera models, the node will rectify the image online if you ",(0,i.jsx)(n.strong,{children:"provide a rectification matrix"})," (see Examples/Stereo/EuRoC.yaml for an example); ",(0,i.jsx)(n.strong,{children:"otherwise, the image must be pre-rectified"}),". For fisheye camera models, no rectification is required because the system uses the raw image directly:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"rosrun ORB_SLAM3 Stereo PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE ONLINE_RECTIFICATION\n"})}),"\n",(0,i.jsx)(n.h3,{id:"run-the-stereo-inertial",children:"Run the Stereo Inertial"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/camera/left/image_raw"})," For stereo input from the subject ",(0,i.jsx)(n.code,{children:"/camera/right/image_raw"})," and inertial input from the subject ",(0,i.jsx)(n.code,{children:"/imu"}),", run the node ORB_SLAM3/Stereo_Inertial. You need to provide a vocabulary file and a settings file, and if needed, a correction matrix, in a similar way to the stereo case:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"rosrun ORB_SLAM3 Stereo_Inertial PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE ONLINE_RECTIFICATION [EQUALIZATION]\n"})}),"\n",(0,i.jsx)(n.h3,{id:"run-the-rgb_d",children:"Run the RGB_D"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/camera/rgb/image_raw"})," For RGB-D input from the subject and the RGB-D image ",(0,i.jsx)(n.code,{children:"/camera/depth_registered/image_raw"}),", run the node ORB_SLAM3/RGBD. You will need to provide a vocabulary file and a settings file. See the RGB-D example above."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"rosrun ORB_SLAM3 RGBD PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE\n"})}),"\n",(0,i.jsxs)(n.p,{children:["**Running ROS examples:** Download a rosbag (e.g. V1_02_medium.bag) from the EuRoC dataset ( ",(0,i.jsx)(n.a,{href:"http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",children:"http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets"})," ). Open three terminal tabs and run the following command in each tab to configure stereo inertial:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"roscore\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"rosrun ORB_SLAM3 Stereo_Inertial Vocabulary/ORBvoc.txt Examples/Stereo-Inertial/EuRoC.yaml true\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"rosbag play --pause V1_02_medium.bag /cam0/image_raw:=/camera/left/image_raw /cam1/image_raw:=/camera/right/image_raw /imu0:=/imu\n"})}),"\n",(0,i.jsx)(n.p,{children:"Once ORB-SLAM3 has loaded the vocabulary, press the spacebar in the rosbag tab."}),"\n",(0,i.jsx)(n.p,{children:"**Note:** For rosbags from the TUM-VI dataset, some playback issues may occur due to block size issues. A possible solution is to repack them using the default block size, for example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-markdown",children:"rosrun rosbag fastrebag.py dataset-room1_512_16.bag dataset-room1_512_16_small_chunks.bag\n"})}),"\n",(0,i.jsx)(n.h2,{id:"8-runtime-analysis",children:"8. Runtime Analysis"}),"\n",(0,i.jsxs)(n.p,{children:["The flag in ",(0,i.jsx)(n.code,{children:"include\\Config.h"})," activates time measurement. This line needs to be uncommented ",(0,i.jsx)(n.code,{children:"#define REGISTER_TIMES"})," to get time statistics for one execution, which are displayed on the terminal and stored in a text file ( ",(0,i.jsx)(n.code,{children:"ExecTimeMean.txt"}),")."]}),"\n",(0,i.jsx)(n.h2,{id:"9-calibration",children:"9. Calibration"}),"\n",(0,i.jsxs)(n.p,{children:["You can find a tutorial for visual-inertial calibration and a detailed description of the contents of valid configuration files at ",(0,i.jsx)(n.code,{children:"Calibration_Tutorial.pdf"})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var a=r(6540);const i={},s=a.createContext(i);function t(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);